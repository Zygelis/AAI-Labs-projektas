{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab22b47e-785f-444c-8cb5-85565c82073e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6_Data_preparation.py done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import import_ipynb\n",
    "\n",
    "# Change working directory for jupyter\n",
    "new_directory = \"C:\\\\Users\\\\Zygis\\\\Desktop\\\\AAI-Labs-projektas\"\n",
    "os.chdir(new_directory)\n",
    "\n",
    "# Read data\n",
    "df = pd.read_excel(\"WEOOct2020all.xls\", engine=\"xlrd\")\n",
    "\n",
    "# Features that are not related to GDP per capita, except 'NGDPDPC' which is the target variable.\n",
    "all_features = [\n",
    "    \"NGDPDPC\",\n",
    "    \"PPPEX\",\n",
    "    \"PCPI\",\n",
    "    \"PCPIE\",\n",
    "    \"TM_RPCH\",\n",
    "    \"TMG_RPCH\",\n",
    "    \"TX_RPCH\",\n",
    "    \"TXG_RPCH\",\n",
    "    \"LUR\",\n",
    "    \"LE\",\n",
    "    \"LP\",\n",
    "    \"GGR\",\n",
    "    \"GGX\",\n",
    "    \"GGXCNL\",\n",
    "    \"GGSB\",\n",
    "    \"GGXONLB\",\n",
    "    \"GGXWDN\",\n",
    "    \"GGXWDG\",\n",
    "    \"BCA\",\n",
    "]\n",
    "\n",
    "\n",
    "# Data preparation\n",
    "def data_preparation(df, features):\n",
    "    df = df.drop(\n",
    "        [\n",
    "            \"Country\",\n",
    "            \"Subject Descriptor\",\n",
    "            \"Units\",\n",
    "            \"Subject Notes\",\n",
    "            \"Country/Series-specific Notes\",\n",
    "            \"Estimates Start After\",\n",
    "            \"Scale\",\n",
    "            \"ISO\",\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Clean strange values that could corrupt data\n",
    "    df = (\n",
    "        df.replace(\"\\n\", np.nan)\n",
    "        .replace(\"\\t\", np.nan)\n",
    "        .replace(\";\", np.nan)\n",
    "        .replace(\"--\", np.nan)\n",
    "    )\n",
    "\n",
    "    # Select rows where WEO Subject Code is features\n",
    "    df = df[df[\"WEO Subject Code\"].isin(features)]\n",
    "\n",
    "    # MODIFYING DATAFRAME STRUCTURE\n",
    "    # Use melt to move years to a single column\n",
    "    df_melted = df.melt(\n",
    "        id_vars=[\"WEO Country Code\", \"WEO Subject Code\"],\n",
    "        var_name=\"Year\",\n",
    "        value_name=\"Value\",\n",
    "    )\n",
    "\n",
    "    # Use pivot to make each feature a column\n",
    "    df_pivoted = df_melted.pivot_table(\n",
    "        index=[\"WEO Country Code\", \"Year\"],\n",
    "        columns=\"WEO Subject Code\",\n",
    "        values=\"Value\",\n",
    "        aggfunc=\"first\",\n",
    "    )\n",
    "\n",
    "    # Reset the index to return index to columns\n",
    "    df_pivoted.reset_index(inplace=True)\n",
    "\n",
    "    # DEALING WITH NAN VALUES\n",
    "    # Drop column if more then 40% values are nan\n",
    "    df_pivoted = df_pivoted.dropna(axis=1, thresh=int(len(df_pivoted) * 0.5))\n",
    "\n",
    "    # Drop row if more then 40% values are nan\n",
    "    df_pivoted = df_pivoted.dropna(axis=0, thresh=int(len(df_pivoted.columns) * 0.5))\n",
    "\n",
    "    # Fill nan values with mean of collumn\n",
    "    df_pivoted = df_pivoted.fillna(df_pivoted.mean())\n",
    "\n",
    "    # Now we have clean data\n",
    "    # Drop \"WEO Country Code\", \"Year\" because they are not needed\n",
    "    df_pivoted = df_pivoted.drop([\"WEO Country Code\", \"Year\"], axis=1)\n",
    "\n",
    "    # To excel\n",
    "    #df_pivoted.to_excel(\"df_cleaned.xlsx\")\n",
    "\n",
    "    # Split data to X and y\n",
    "    df_X = df_pivoted.drop([\"NGDPDPC\"], axis=1)\n",
    "\n",
    "    df_y = df_pivoted[\"NGDPDPC\"]\n",
    "\n",
    "    return df_X, df_y\n",
    "\n",
    "\n",
    "def data_split(df_X, df_y):\n",
    "    # Split data to train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, random_state=0)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def data_scale(df_X):\n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df_X)\n",
    "\n",
    "    return X_scaled\n",
    "\n",
    "\n",
    "# Feature selection (mutual information)\n",
    "def make_mi_scores(X, y):\n",
    "    # Create discrete features for mutual information calculation\n",
    "    discrete_features = X.dtypes == int\n",
    "\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "\n",
    "def plot_mi_scores(mi_scores):\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    ax = sns.barplot(x=mi_scores, y=mi_scores.index)\n",
    "    ax.set_title(\"Mutual Information Scores\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "df_X, df_y = data_preparation(df, all_features)\n",
    "\n",
    "mi_scores = make_mi_scores(df_X, df_y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = data_split(df_X, df_y)\n",
    "\n",
    "print(\"6_Data_preparation.py done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26916b0a-6eeb-44c0-9371-8fdc58c2e5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000340 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3825\n",
      "[LightGBM] [Info] Number of data points in the train set: 5891, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 10189.517567\n",
      "\n",
      "\n",
      "Models_6.py done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Read data\n",
    "df = pd.read_excel(\"C:\\\\Users\\\\Zygis\\\\Desktop\\\\test\\\\WEOOct2020all.xls\", engine=\"xlrd\")\n",
    "\n",
    "# Prepare data\n",
    "df_X, df_y = data_preparation(df, all_features)\n",
    "\n",
    "# Split data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, random_state=0)\n",
    "\n",
    "\n",
    "# Define 3 models:\n",
    "# XGBoost model\n",
    "def XGB_model_fit(X_train, y_train, X_test, y_test):\n",
    "    # Train model\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=1000, learning_rate=0.05, n_jobs=4, random_state=0\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        early_stopping_rounds=10,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # Get predictions\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# LightGBM model\n",
    "def LGBM_model_fit(X_train, y_train):\n",
    "    # Train model\n",
    "    model = LGBMRegressor(\n",
    "        n_estimators=1000, learning_rate=0.05, n_jobs=4, random_state=0\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Get predictions\n",
    "    predictions = model.predict(X_train)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Random Forest model\n",
    "def RF_model_fit(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "    # Select categorical columns with low unique values\n",
    "    categorical_cols = [\n",
    "        cname\n",
    "        for cname in X_train.columns\n",
    "        if X_train[cname].nunique() < 10 and X_train[cname].dtype == \"object\"\n",
    "    ]\n",
    "\n",
    "    # Select numerical columns\n",
    "    numerical_cols = [\n",
    "        cname\n",
    "        for cname in X_train.columns\n",
    "        if X_train[cname].dtype in [\"int64\", \"float64\"]\n",
    "    ]\n",
    "\n",
    "    # Keep selected columns only\n",
    "    my_cols = categorical_cols + numerical_cols\n",
    "    X_train = X_train[my_cols].copy()\n",
    "    X_test = X_test[my_cols].copy()\n",
    "\n",
    "    # Preprocessing for numerical data\n",
    "    numerical_transformer = SimpleImputer(strategy=\"mean\")\n",
    "\n",
    "    # Preprocessing for categorical data\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create one preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numerical_transformer, numerical_cols),\n",
    "            (\"cat\", categorical_transformer, categorical_cols),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Define model\n",
    "    model = RandomForestRegressor(n_estimators=900, random_state=0, n_jobs=4)\n",
    "\n",
    "    # Create pipeline with preprocessor and ML model\n",
    "    RF_pipe = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"model\", model)])\n",
    "    RF_pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Get predictions\n",
    "    prediction = RF_pipe.predict(X_test)\n",
    "\n",
    "    return RF_pipe\n",
    "\n",
    "\n",
    "# Fit and define models\n",
    "model_xgb = XGB_model_fit(X_train, y_train, X_test, y_test)\n",
    "model_lgbm = LGBM_model_fit(X_train, y_train)\n",
    "model_rf = RF_model_fit(df_X, df_y)\n",
    "\n",
    "\n",
    "# Model quality evaluation\n",
    "def perform_cross_validation(model, X, y, num_folds=5):\n",
    "    # Initialize KFold cross-validator\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=0)\n",
    "\n",
    "    # Perform cross-validation and calculate MAE and MSE scores\n",
    "    scores_mae = -1 * cross_val_score(\n",
    "        model, X, y, cv=kf, scoring=\"neg_mean_absolute_error\"\n",
    "    )  # Multiply by -1 to Convert back to positive\n",
    "\n",
    "    scores_mse = -1 * cross_val_score(\n",
    "        model, X, y, cv=kf, scoring=\"neg_mean_squared_error\"\n",
    "    )  # Multiply by -1 to Convert back to positive4\n",
    "\n",
    "    # Calculate the mean of MAE and MSE scores\n",
    "    mean_mae = scores_mae.mean()\n",
    "\n",
    "    mean_mse = scores_mse.mean()\n",
    "\n",
    "    return mean_mae, mean_mse\n",
    "\n",
    "\n",
    "# Compare models and select the best one\n",
    "def compare_models(X, y):\n",
    "    models = {\"XGB\": model_xgb, \"LGBM\": model_lgbm, \"RF\": model_rf}\n",
    "\n",
    "    results_mae = {}\n",
    "    results_mse = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        mean_mae, mean_mse = perform_cross_validation(model, X, y)\n",
    "        results_mae[name] = mean_mae\n",
    "        results_mse[name] = mean_mse\n",
    "\n",
    "    best_model_mae = min(results_mae, key=results_mae.get)\n",
    "    best_model_mse = min(results_mse, key=results_mse.get)\n",
    "\n",
    "    print(\"\\n\" * 2)\n",
    "    print(\"Model comparison results:\")\n",
    "\n",
    "    for name, mae_score in results_mae.items():\n",
    "        mse_score = results_mse[name]\n",
    "        print(f\"{name}: MAE = {mae_score:.4f}, MSE = {mse_score:.4f}\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\n",
    "        f\"Best model based on MAE: {best_model_mae} with MAE = {results_mae[best_model_mae]:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Best model based on MSE: {best_model_mse} with MSE = {results_mse[best_model_mse]:.4f}\"\n",
    "    )\n",
    "\n",
    "    return models[best_model_mse]\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Models_6.py done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cae8756f-ccf9-4321-9e43-dcdecc6c5787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "A)\n",
      "Prediction error on the training and the testing data sets for the best model with all features:\n",
      "mae_test: 1700.374663079769\n",
      "mse_test: 9467537.595610352\n",
      "mae_train: 427.3895685040062\n",
      "mse_train: 419284.59170429525\n",
      "\n",
      "\n",
      "B)\n",
      "Fields used in the model:\n",
      "['NGDPDPC', 'PPPEX', 'PCPI', 'PCPIE', 'TM_RPCH', 'TMG_RPCH', 'TX_RPCH', 'TXG_RPCH', 'LUR', 'LE', 'LP', 'GGR', 'GGX', 'GGXCNL', 'GGSB', 'GGXONLB', 'GGXWDN', 'GGXWDG', 'BCA']\n",
      "\n",
      "\n",
      "C)\n",
      "Top 5 fields/features that contribute the most to the predictions:\n",
      "WEO Subject Code\n",
      "BCA       0.299262\n",
      "LP        0.252006\n",
      "LUR       0.244358\n",
      "PPPEX     0.216711\n",
      "GGXWDG    0.145016\n",
      "Name: MI Scores, dtype: float64\n",
      "\n",
      "\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000154 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 6167, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 10038.225264\n",
      "\n",
      "\n",
      "D)\n",
      "Prediction error on the training and the testing data sets with top 5 features\n",
      "mae_test: 1837.0738538558217\n",
      "mse_test: 10605175.511810161\n",
      "mae_train: 859.5551773622467\n",
      "mse_train: 1840347.431103236\n",
      "\n",
      "\n",
      "E)\n",
      "Saved the model as:\n",
      "model_top5.joblib\n",
      "\n",
      "\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000227 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2040\n",
      "[LightGBM] [Info] Number of data points in the train set: 6129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 9943.298448\n",
      "\n",
      "\n",
      "F)\n",
      "Prediction error on the training and the testing data sets for the best model with minimum error:\n",
      "mae_test: 1554.0441283812893\n",
      "mse_test: 8150637.219803915\n",
      "mae_train: 607.3156611179496\n",
      "mse_train: 903706.7489603574\n",
      "\n",
      "\n",
      "Fields used in the model with minimum error:\n",
      "['NGDPDPC', 'BCA', 'LP', 'LUR', 'GGXWDG', 'GGX', 'GGR', 'PPPEX', 'GGXCNL']\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Determine the best model overall\n",
    "# This usually takes a few minutes to run. The best model is LGBM.\n",
    "#best_model = compare_models(df_X, df_y)\n",
    "# If you want to run code faster, comment out the line above and uncomment the line below.\n",
    "best_model = model_lgbm\n",
    "\n",
    "# Best model prediction\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "\n",
    "# Prediction error on the training and the testing data sets for the best model\n",
    "print(\"\\n\")\n",
    "print('A)')\n",
    "print(\"Prediction error on the training and the testing data sets for the best model with all features:\")\n",
    "\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "print(\"mae_test:\", mae_test)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "print(\"mse_test:\", mse_test)\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "print(\"mae_train:\", mae_train)\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "print(\"mse_train:\", mse_train)\n",
    "\n",
    "\n",
    "# Fields used in the model (all_features)\n",
    "print(\"\\n\")\n",
    "print('B)')\n",
    "print(\"Fields used in the model:\")\n",
    "print(all_features)\n",
    "\n",
    "\n",
    "# Top 5 fields/features that contribute the most to the predictions\n",
    "mi_scores = make_mi_scores(df_X, df_y)\n",
    "top_5_features = mi_scores.head(5)\n",
    "print(\"\\n\")\n",
    "print('C)')\n",
    "print(\"Top 5 fields/features that contribute the most to the predictions:\")\n",
    "print(top_5_features)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Make top 5 features codes a list\n",
    "top_5_features = top_5_features.index.tolist()\n",
    "\n",
    "# Add NGDPDPC to top_5_features as a target\n",
    "top_5_features.append(\"NGDPDPC\")\n",
    "\n",
    "# Train another predictor that uses those top 5 features\n",
    "# Prepare data\n",
    "df_X_top5, df_y_top5 = data_preparation(df, top_5_features)\n",
    "\n",
    "# Split data\n",
    "X_train_top5, X_test_top5, y_train_top5, y_test_top5 = train_test_split(df_X_top5, df_y_top5, random_state=0)\n",
    "\n",
    "# Train another predictor that uses those top 5 features\n",
    "model_top5 = LGBM_model_fit(X_train_top5, y_train_top5)\n",
    "\n",
    "# Best model prediction\n",
    "y_test_pred_top5 = model_top5.predict(X_test_top5)\n",
    "y_train_pred_top5 = model_top5.predict(X_train_top5)\n",
    "\n",
    "# Prediction error on the training and the testing data sets\n",
    "print(\"\\n\")\n",
    "print('D)')\n",
    "print(\"Prediction error on the training and the testing data sets with top 5 features\")\n",
    "mae_test_top5 = mean_absolute_error(y_test_top5, y_test_pred_top5)\n",
    "print(\"mae_test:\", mae_test_top5)\n",
    "mse_test_top5 = mean_squared_error(y_test_top5, y_test_pred_top5)\n",
    "print(\"mse_test:\", mse_test_top5)\n",
    "mae_train_top5 = mean_absolute_error(y_train_top5, y_train_pred_top5)\n",
    "print(\"mae_train:\", mae_train_top5)\n",
    "mse_train_top5 = mean_squared_error(y_train_top5, y_train_pred_top5)\n",
    "print(\"mse_train:\", mse_train_top5)\n",
    "\n",
    "\n",
    "# Save the model\n",
    "print(\"\\n\")\n",
    "print('E)')\n",
    "print(\"Saved the model as:\")\n",
    "print(\"model_top5.joblib\")\n",
    "print(\"\\n\")\n",
    "\n",
    "save_path = \"6-dalis\\\\model_top5.joblib\"\n",
    "joblib.dump(model_top5, save_path)\n",
    "\n",
    "# Load the model\n",
    "model_loaded = joblib.load(\"6-dalis\\\\model_top5.joblib\")\n",
    "\n",
    "\n",
    "# Predictor with best MAE and MSE scores that I have found:\n",
    "# Features used in the model\n",
    "features_min_error = ['NGDPDPC', 'BCA', 'LP', 'LUR', 'GGXWDG', 'GGX', 'GGR', 'PPPEX', 'GGXCNL']\n",
    "\n",
    "# Prepare data\n",
    "df_X, df_y = data_preparation(df, features_min_error)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, random_state=0)\n",
    "\n",
    "# Train model\n",
    "model_min_error = LGBM_model_fit(X_train, y_train)\n",
    "\n",
    "# Predictions for test and train data\n",
    "y_test_pred_min_error = model_min_error.predict(X_test)\n",
    "y_train_pred_min_error = model_min_error.predict(X_train)\n",
    "\n",
    "# Prediction error\n",
    "mae_test_min_error = mean_absolute_error(y_test, y_test_pred_min_error)\n",
    "mse_test_min_error = mean_squared_error(y_test, y_test_pred_min_error)\n",
    "mae_train_min_error = mean_absolute_error(y_train, y_train_pred_min_error)\n",
    "mse_train_min_error = mean_squared_error(y_train, y_train_pred_min_error)\n",
    "\n",
    "print(\"\\n\")\n",
    "print('F)')\n",
    "print(\"Prediction error on the training and the testing data sets for the best model with minimum error:\")\n",
    "print(\"mae_test:\", mae_test_min_error)\n",
    "print(\"mse_test:\", mse_test_min_error)\n",
    "print(\"mae_train:\", mae_train_min_error)\n",
    "print(\"mse_train:\", mse_train_min_error)\n",
    "print(\"\\n\")\n",
    "print(\"Fields used in the model with minimum error:\")\n",
    "print(features_min_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
